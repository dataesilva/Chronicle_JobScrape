{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "alljobssoups = []\n",
    "alljoburls = []\n",
    "alljoburlstems = []\n",
    "\n",
    "#Determine how many pages of ads are on https://jobs.chronicle.com/jobs\n",
    "#When collected, was 665, now 852\n",
    "#For up-to-minute, use RSS feed\n",
    "\n",
    "for i in range(1, 665):\n",
    "    #print(i)\n",
    "    page=requests.get('https://jobs.chronicle.com/jobs/'+str(i)+'/')\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    alljobssoups.append(soup)\\\n",
    "    for a in soup.find_all('a', attrs={'class': 'js-clickable-area-link'}):\n",
    "        alljoburls.append('https://jobs.chronicle.com'+str(a.get('href').replace(\" \\r\\n\\t\", \"\").replace(\"\\r\\n\\r\\n\\r\\n\\r\\n\", \"\")))\n",
    "        alljoburlstems.append(a.get('href').replace(\" \\r\\n\\t\", \"\").replace(\"\\r\\n\\r\\n\\r\\n\\r\\n\", \"\"))\n",
    "    print(str(i)+' is done. '+str(664-i)+' are left.')\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldescsoups = []\n",
    "alldesctitle = []\n",
    "employ = []\n",
    "loc = []\n",
    "salary = []\n",
    "date = []\n",
    "categories = []\n",
    "level = []\n",
    "employtype = []\n",
    "text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check length of alljoburls list for range\n",
    "for i in range(0,13267):\n",
    "    page = requests.get(alljoburls[i])\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    alldescsoups.append(soup)\n",
    "    #number of 'dd' tags determines number of job info fields. 7 is max available fields\n",
    "    if len(soup.find_all('dd')) == 7:\n",
    "        employ.append(soup.find_all('dd')[0].get_text().replace(\"\\n\", \"\"))\n",
    "        loc.append(soup.find_all('dd')[1].get_text().replace(\"\\n\", \"\"))\n",
    "        salary.append(soup.find_all('dd')[2].get_text().replace(\"\\n\", \"\"))\n",
    "        date.append(soup.find_all('dd')[3].get_text().replace(\"\\n\", \"\"))\n",
    "        categories.append(soup.find_all('dd')[4].get_text().replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\", \\n\\n\", \"; \").replace(\"\\n\", \"\"))\n",
    "        level.append(soup.find_all('dd')[5].get_text().replace(\"\\n\", \"\"))\n",
    "        employtype.append(soup.find_all('dd')[6].get_text().replace(\"\\n\", \"\"))\n",
    "        text.append(soup.find_all(attrs = {'class': 'block fix-text job-description'}))\n",
    "        for a in soup.find_all('h1'):\n",
    "            alldesctitle.append(a.get_text())\n",
    "    #'dd' == 6 is when the \"level\" field is missing, which is usually for admin positions\n",
    "    elif len(soup.find_all('dd')) == 6:\n",
    "        employ.append(soup.find_all('dd')[0].get_text().replace(\"\\n\", \"\"))\n",
    "        loc.append(soup.find_all('dd')[1].get_text().replace(\"\\n\", \"\"))\n",
    "        salary.append(soup.find_all('dd')[2].get_text().replace(\"\\n\", \"\"))\n",
    "        date.append(soup.find_all('dd')[3].get_text().replace(\"\\n\", \"\"))\n",
    "        categories.append(soup.find_all('dd')[4].get_text().replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\", \\n\\n\", \"; \").replace(\"\\n\", \"\"))\n",
    "        level.append('Not Applicable')\n",
    "        employtype.append(soup.find_all('dd')[5].get_text().replace(\"\\n\", \"\"))\n",
    "        text.append(soup.find_all(attrs = {'class': 'block fix-text job-description'}))\n",
    "        for a in soup.find_all('h1'):\n",
    "            alldesctitle.append(a.get_text())\n",
    "    #Any other cases are treated as an error\n",
    "    else:\n",
    "        employ.append('error')\n",
    "        loc.append('error')\n",
    "        salary.append('error')\n",
    "        date.append('error')\n",
    "        categories.append('error')\n",
    "        level.append('error')\n",
    "        employtype.append('error')\n",
    "        text.append(soup.find_all(attrs = {'class': 'block fix-text job-description'}))\n",
    "        for a in soup.find_all('h1'):\n",
    "            alldesctitle.append(a.get_text())\n",
    "    #Progress statement\n",
    "    print(str(i+1)+' is done.'+str(13267-i+1)+' remain.')\n",
    "    #Delayed to not overly tax server\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build one DataFrame from all lists. Will throw an error here if something is wrong.\n",
    "alljobdat = pd.DataFrame({'title': alldesctitle, 'employer': employ, 'location': loc, 'salary': salary, 'post_date': date, 'categories': categories, 'pos_type': level, 'employ_type': employtype, 'description': text, 'url': alljoburls})\n",
    "#Export CSV with data\n",
    "alljobdat.to_csv('chronicle_jobs_utf8.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
